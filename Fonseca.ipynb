{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import seaborn as sbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Baseline\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "def drop_nulls(train_data):\n",
    "    return train_data[train_data['Request'].notnull()]\n",
    "\n",
    "train = drop_nulls(train)\n",
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "train_X, train_y = train_data['Request'], train_data['Label']\n",
    "val_X, val_y = val_data['Request'], val_data['Label']\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,3))),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "text_clf.fit(train_X, train_y)\n",
    "\n",
    "accuracy_score(val_y, text_clf.predict(val_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.loc[:, self.keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length\n",
    "train['len_message'] = train['Request'].apply(lambda x: len(x))\n",
    "test['len_message'] = test['Request'].apply(lambda x: len(x))\n",
    "\n",
    "# Number of words\n",
    "train['n_words'] = train['Request'].str.split().map(len)\n",
    "test['n_words'] = test['Request'].str.split().map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm', disable=['parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    n_adj = []\n",
    "    n_verbs = []\n",
    "    len_message = []\n",
    "\n",
    "    for doc in nlp.pipe(df['Request']):\n",
    "        n_adj.append(len([token for token in doc if token.pos_ == 'ADJ']))\n",
    "        n_verbs.append(len([token for token in doc if token.pos_ == 'VERB']))\n",
    "        \n",
    "    df['n_adj'] = n_adj\n",
    "    df['n_verbs'] = n_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "train_X, train_y = train_data.drop('Label', axis=1), train_data['Label']\n",
    "val_X, val_y = val_data.drop('Label', axis=1), val_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = Pipeline([\n",
    "    ('select', TextSelector(key='Request')),\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,1), stop_words=stopwords))\n",
    "])\n",
    "\n",
    "twoplus = Pipeline([\n",
    "    ('select', TextSelector(key='Request')),\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(2,3), max_features=10000))])\n",
    "\n",
    "length = Pipeline([\n",
    "    ('select', NumberSelector(key='len_message')),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "\n",
    "n_words = Pipeline([\n",
    "    ('select', NumberSelector(key='n_words')),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "\n",
    "n_adj = Pipeline([\n",
    "    ('select', NumberSelector(key='n_adj')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "n_verbs = Pipeline([\n",
    "    ('select', NumberSelector(key='n_verbs')),\n",
    "    ('scale', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_dict = {'add_to_playlist':  ['adicion','faixa', 'lista', 'música','colocar', 'reprodução','clássicos','compilacao','melodia','artista','disco'],\n",
    "            'search_screening_event': ['curta','longa','metragem','filme', 'telefilme','teatro','cinema','horas','animação','documentario'],\n",
    "            'book_restaurant':  ['restaurante','mesa','reserva','comer','lugar','bar'],\n",
    "            'rate_book': ['de 6','saga','estrelas','livro','classificar','romance','encoclopédia','texto','dar','valor','título','0','1','2','3','4','5','6','um','dois','três','quatro','cinco','seis'],\n",
    "            'get_weather': ['área','posição','tempestade','clima','previsão','neve','tempo','frio','chuva','calor','vento','temperatura','neblina','sol','nublado'],\n",
    "            'play_music':  ['álbum','artista','spotify','melodia','balada','música','anos','youtube','setenta','oitenta','noventa','ouvir','reproduzir','tocar','sinfonia','polular','metal','rock','jazz','sonora','punk','blues','clássica','quarenta','cinquenta','sessenta','setenta','oitenta','noventa','ópera'],\n",
    "            'search_creative_work':  ['procure','encontre','pesquisar','arranjar','video','televisão','título','gostaria','programa','trabalho','criativo','quero','mostrar','obter','pintura','filme','show','vídeo','game','jogo','foto','achar'],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_stem_dict(original_dict):\n",
    "    stem_dict = {}\n",
    "    for k in original_dict:\n",
    "        stem_dict[k] = list(map(stemmer.stem, original_dict[k]))\n",
    "    return stem_dict\n",
    "\n",
    "stem_dict = create_stem_dict(key_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    for k in stem_dict:\n",
    "        words = []\n",
    "        for doc in df['Request']:\n",
    "            words.append(len([word for word in doc.split() if stemmer.stem(word) in stem_dict[k]]))\n",
    "        df[k] = words / df['len_message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "train_X, train_y = train_data.drop('Label', axis=1), train_data['Label']\n",
    "val_X, val_y = val_data.drop('Label', axis=1), val_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_feats = Pipeline([\n",
    "    ('select', MultipleColumnSelector(list(key_dict.keys()))),\n",
    "    ('scale', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = FeatureUnion([\n",
    "    ('one', one),\n",
    "    ('twop', twoplus),\n",
    "    ('len', length),\n",
    "    ('n_words', n_words),\n",
    "    ('n_adj', n_adj),\n",
    "    ('n_verbs', n_verbs),\n",
    "    ('stem_feats', stem_feats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('feat', feats),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "#pipe.fit(train_X, train_y)\n",
    "#accuracy_score(val_y, pipe.predict(val_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pipe = Pipeline(pipe.steps[:-1])\n",
    "train_no_emb = pd.DataFrame(t_pipe.fit_transform(train).to_array())\n",
    "test_no_emb = pd.DataFrame(t_pipe.transform(test).to_array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding = pd.read_csv('train_embeddings.csv',index_col=0)\n",
    "test_embedding = pd.read_csv('test_embeddings.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_embedding, train_no_emb, train['Label']], axis=1)\n",
    "test_data = pd.concat([test_embedding, test_no_emb], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "train_X, train_y = train_data.drop('Label', axis=1), train_data['Label']\n",
    "val_X, val_y = val_data.drop('Label', axis=1), val_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC().fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8859523809523809"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(val_y, svm.predict(val_X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ldsa_updated]",
   "language": "python",
   "name": "conda-env-ldsa_updated-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
